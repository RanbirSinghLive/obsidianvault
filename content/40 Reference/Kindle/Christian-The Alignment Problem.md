---
kindle-sync:
  bookId: '53793'
  title: 'The Alignment Problem: Machine Learning and Human Values'
  author: Brian Christian
  asin: B085T55LGK
  lastAnnotatedDate: '2025-04-17'
  bookImageUrl: 'https://m.media-amazon.com/images/I/91AyH08s4JL._SY160.jpg'
  highlightsCount: 32
---
# The Alignment Problem
## Metadata
* Author: [Brian Christian](https://www.amazon.comundefined)
* ASIN: B085T55LGK
* Reference: https://www.amazon.com/dp/B085T55LGK
* [Kindle link](kindle://book?action=open&asin=B085T55LGK)

## Highlights
It was already known by the early 1940s that the brain is built of neurons wired together, and that each neuron has “inputs” (dendrites) as well as an “output” (axon). When the impulses coming into a neuron exceed a certain threshold, then that neuron, in turn, emits a pulse. Immediately this begins to feel, to McCulloch and Pitts, like logic: the pulse or its absence signifying on or off, yes or no, true or false.7 They realize that a neuron with a low-enough threshold, such that it would fire if any of its inputs did, functioned like a physical embodiment of the logical or. A neuron with a high-enough threshold, such that it would only fire if all of its inputs did, was a physical embodiment of the logical and. There was nothing, then, that could be done with logic—they start to realize—that such a “neural network,” so long as it was wired appropriately, could not do. Within months they have written a paper together—the middle-aged neurologist and teenage logician. They call it “A Logical Calculus of Ideas Immanent in Nervous Activity.” — location: [84](kindle://book?action=open&asin=B085T55LGK&location=84) ^ref-47683

---
No one realized what the problem was for two years. In November 2015, Boston University PhD student Tolga Bolukbasi went with his advisor to a Friday happy-hour meeting at Microsoft Research. Amid wine sipping and informal chat, he and Microsoft researcher Adam Kalai pulled out their laptops and started messing around with word2vec. “We were playing around with these word embeddings, and we just started randomly putting words into it,” Bolukbasi says. “I was playing on my PC; Adam started playing.”3 Then something happened. They typed: doctor − man + woman The answer came back: nurse “We were shocked at that point, and we realized there was a problem,” says Kalai. “And then we dug deeper and saw that it was even worse than that.”4 The pair tried another. shopkeeper − man + woman The answer came back: housewife They tried another. computer programmer − man + woman Answer: homemaker Other conversations in the room by this point had stopped, and a group had formed around the screen. “We jointly realized,” says Bolukbasi, “Hey, there’s something wrong here.” — location: [130](kindle://book?action=open&asin=B085T55LGK&location=130) ^ref-23084

---
Amodei had made the oldest mistake in the book: “rewarding A, while hoping for B.” — location: [205](kindle://book?action=open&asin=B085T55LGK&location=205) ^ref-56972

---
what we don’t want—is difficult to state directly or completely. — location: [221](kindle://book?action=open&asin=B085T55LGK&location=221) ^ref-49522

---
The field of machine learning comprises three major areas: In unsupervised learning, a machine is simply given a heap of data and—as with the word2vec system—told to make sense of it, to find patterns, regularities, useful ways of condensing or representing or visualizing it. In supervised learning, the system is given a series of categorized or labeled examples—like parolees who went on to be rearrested and others who did not—and told to make predictions about new examples it hasn’t seen yet, or for which the ground truth is not yet known. And in reinforcement learning, the system is placed into an environment with rewards and punishments—like the boat-racing track with power-ups and hazards—and told to figure out the best way to minimize the punishments and maximize the rewards. — location: [228](kindle://book?action=open&asin=B085T55LGK&location=228) ^ref-2200

---
Machine learning is an ostensibly technical field crashing increasingly on human questions. Our human, social, and civic dilemmas are becoming technical. And our technical dilemmas are becoming human, social, and civic. Our successes and failures alike in getting these systems to do “what we want,” it turns out, offer us an unflinching, revelatory mirror. — location: [270](kindle://book?action=open&asin=B085T55LGK&location=270) ^ref-61933

---
Says Rosenblatt to the New Yorker reporter, “Our success in developing the perceptron means that for the first time a non-biological object will achieve an organization of its external environment in a meaningful way. That’s a safe definition of what the perceptron can do. My colleague disapproves of all the loose talk one hears nowadays about mechanical brains. He prefers to call our machine a self-organizing system, but, between you and me, that’s precisely what any brain is.” — location: [329](kindle://book?action=open&asin=B085T55LGK&location=329) ^ref-51960

---
Pitts dies from cirrhosis in May 1969, at the age of 46.9 A few months later Warren McCulloch, at the age of 70, succumbs to a heart seizure after a long series of cardiopulmonary problems. In 1971, while celebrating his 43rd birthday, Frank Rosenblatt drowns in a sailing accident on the Chesapeake Bay. — location: [361](kindle://book?action=open&asin=B085T55LGK&location=361) ^ref-9291

Dramatic end to the birth of neural nets

---
By the late ’80s and early ’90s, a former postdoc of Hinton’s named Yann LeCun, working at Bell Labs, had trained neural networks to identify handwritten numerals from 0 to 9, and neural networks found their first major commercial use: reading zip codes in post offices, and deposit checks in ATMs.14 By the 1990s, LeCun’s networks were processing 10 to 20% of all checks in the United States.15 — location: [376](kindle://book?action=open&asin=B085T55LGK&location=376) ^ref-11247

---
It was understood that, in principle, a big-enough neural network, with enough training examples and time, can learn almost anything.16 But no one had fast-enough computers, enough data to train on, or enough patience to make good on that theoretical potential. — location: [382](kindle://book?action=open&asin=B085T55LGK&location=382) ^ref-15223

---
In 2005, Amazon launched its “Mechanical Turk” service, allowing for the recruiting of human labor on a large scale, making it possible to hire thousands of people to perform simple actions for pennies a click. (The service was particularly well suited to the kinds of things that future AI is thought to be able to do—hence its tagline: artificial artificial intelligence.) — location: [390](kindle://book?action=open&asin=B085T55LGK&location=390) ^ref-59256

---
When Frank Rosenblatt was interviewed about his perceptron in 1958, he was asked what practical or commercial uses a machine like the perceptron might have. “At the moment, none whatever,” he replied cheerfully.21 “In these matters, you know, use follows invention.” — location: [438](kindle://book?action=open&asin=B085T55LGK&location=438) ^ref-38309

---
As UC Berkeley’s Moritz Hardt argues, “The whole spiel about big data is that we can build better classifiers largely as a result of having more data. The contrapositive is that less data leads to worse predictions. Unfortunately, it’s true by definition that there is always proportionately less data available about minorities. This means that our models about minorities generally tend to be worse than those about the general population.”37 — location: [525](kindle://book?action=open&asin=B085T55LGK&location=525) ^ref-36567

---
Bias in machine-learning systems is often a direct result of the data on which the systems are trained—making it incredibly important to understand who is represented in those datasets, and to what degree, before using them to train systems that will affect real people. — location: [613](kindle://book?action=open&asin=B085T55LGK&location=613) ^ref-8397

---
Claude Shannon founded information theory in the 1940s on a mathematical analysis of this very sort, noticing that some missing words are more predictable than others, and attempting to quantify by how much.55 — location: [632](kindle://book?action=open&asin=B085T55LGK&location=632) ^ref-45554

---
Early methods involved what are known as “n-grams,” which meant simply counting up every single chain of, say, two words in a row that appeared in a particular corpus—“appeared in,” “in a,” “a particular,” “particular corpus”—and tallying them in a huge database.56 Then it was simple enough, given a missing word, to look at the preceding word and find which n-gram in the database beginning with that preceding word had appeared most often. That would then be your best guess as to what was missing. Of course, additional context beyond just the immediately preceding word could offer you additional clues, but incorporating it was far from straightforward. — location: [634](kindle://book?action=open&asin=B085T55LGK&location=634) ^ref-40530

---
The hypothesis here, the big bet on which the model rests, is simply this: Words will tend to be found near words that are “similar” to themselves. And these similarities can be captured numerically. The neural network model works by transforming (“embedding”) every word into a set (a “vector”) of numbers that represent its “coordinates” in that space. This set of coordinate numbers is known as the word’s representation. (In the case of word2vec, it’s three hundred decimal numbers between −1.0 and 1.0.) This enables a direct measure of how “similar” any word is to any other: How far away are those coordinates? 61 — location: [655](kindle://book?action=open&asin=B085T55LGK&location=655) ^ref-18965

---
“At this point,” explains Stanford computational linguist Christopher Manning, “sort of a miracle occurs.” In his words: It’s sort of surprising—but true—that you can do no more than set up this kind of prediction objective, make it the job of every word’s word vectors to be such that they’re good at predicting the words that appear in their context or vice-versa—you just have that very simple goal—and you say nothing else about how this is going to be achieved—but you just pray and depend on the magic of deep learning. . . . And this miracle happens. And out come these word vectors that are just amazingly powerful at representing the meaning of words and are useful for all sorts of things.63 In fact, one could argue that these embeddings actually manage to capture too much of the nuance of our language. Indeed, they capture with startling clarity the parts we ourselves prefer not to see. — location: [668](kindle://book?action=open&asin=B085T55LGK&location=668) ^ref-54352

---
the embeddings, simple as they are—just a row of numbers for each word, based on predicting nearby missing words in a text—seemed to capture a staggering amount of real-world information. You could, for instance, simply add two vectors together to get a new vector, and search for the nearest word. The results, as we have seen, often made a shocking amount of sense: Czech + currency = koruna Vietnam + capital = Hanoi German + airlines = Lufthansa French + actress = Juliette Binoche* And you could subtract words, too. This meant—incredibly—you could produce “analogies” by getting the “difference” between two words and then “adding” it to a third.66 These analogies suggested that the embeddings had captured geography: Berlin − Germany + Japan = Tokyo And grammar: bigger − big + cold = colder And cuisine: sushi − Japan + Germany = bratwurst And science: Cu − copper + gold = Au And tech: Windows − Microsoft + Google = Android — location: [686](kindle://book?action=open&asin=B085T55LGK&location=686) ^ref-63357

---
Unfortunately, as we’ve seen, that wasn’t all the vectors captured. They contained stunning gender biases. For every clever or apt analogy for man:woman, like fella:babe, or prostate cancer:ovarian cancer, there was a host of others that seemed to be reflecting mere stereotypes, like carpentry:sewing, or architect:interior designer, or doctor:nurse. — location: [704](kindle://book?action=open&asin=B085T55LGK&location=704) ^ref-43098

---
Word2vec maps proper names to racial and gender axes just like it does with any other words, putting Sarah − Matthew on a gender axis and Sarah − Kiesha on a race axis. — location: [724](kindle://book?action=open&asin=B085T55LGK&location=724) ^ref-3787

---
The team of five computer scientists found themselves doing, in effect, social science. Indeed, part of the project ended up requiring consultation beyond their normal disciplinary lines. “We’re a bunch of machine-learning researchers,” says Kalai. “I work in a lab that includes a bunch of social scientists, and just from listening to them talk about various issues that come up in sociology and social sciences, we were aware of these possible concerns that the machine-learning algorithms might discriminate, but none of the five of us—we’re all five guys—had ever worked or read much about gender bias.” — location: [772](kindle://book?action=open&asin=B085T55LGK&location=772) ^ref-64555

---
The Mechanical Turk workers had reported that 19% of the original model’s gender analogies reflected gender stereotypes; of the new, debiased model’s analogies, only 6% were judged to reflect stereotypes.82 This neutralization came at a small cost—the model now, for instance, thought it just as likely that someone could be “grandmothered in” as “grandfathered in” to a legal exemption.83 But maybe this was a price worth paying—and you could always decide how much prediction error you were willing to trade for how much debiasing and set an appropriate tradeoff. — location: [802](kindle://book?action=open&asin=B085T55LGK&location=802) ^ref-55442

---
“One perspective on bias in word embeddings is that it merely reflects bias in society, and therefore one should attempt to debias society rather than word embeddings. However, . . . in a small way debiased word embeddings can hopefully contribute to reducing gender bias in society. At the very least, machine learning should not be used to inadvertently amplify these biases, as we have seen can naturally happen.” — location: [808](kindle://book?action=open&asin=B085T55LGK&location=808) ^ref-8626

---
In a recruiting application these biases may simply be dangers to be mitigated, but taken on their own they raise a host of questions. For instance, where do they come from? Are they an artifact of the statistical technique used, or do they reflect something deeper: namely, the bias in our own heads and the bias in the world at large? — location: [826](kindle://book?action=open&asin=B085T55LGK&location=826) ^ref-260

---
A team of computer scientists at Princeton—postdoc Aylin Caliskan and professors Joanna Bryson and Arvind Narayanan—found that the distance between embeddings in word2vec and other widely used word-embedding models uncannily mirrors this human reaction-time data. The slower people are to identify any two groups of words, the farther away those word vectors were in the model.89 The model’s biases, in other words, are, for better or worse, very much our own. — location: [841](kindle://book?action=open&asin=B085T55LGK&location=841) ^ref-24990

---
The fact that the embeddings that emerge from this “magical” optimization process are so uncannily and discomfitingly useful as a mirror for society means that we have, in effect, added a diagnostic tool to the arsenal of social science. We can use these embeddings to quantify something in precise detail about society at a given snapshot in time. And regardless of causation—whether it’s changes in the objective reality that change the way we speak, or vice versa, or whether both are driven by some deeper cause—we can use these snapshots to watch society change. — location: [856](kindle://book?action=open&asin=B085T55LGK&location=856) ^ref-29465

---
One might imagine a kind of real-time dashboard of whether society itself—or, at the very least, our public discourse—appears to be getting more or less biased: a bellwether for the shifts underway, and a glimpse of the world to come. — location: [882](kindle://book?action=open&asin=B085T55LGK&location=882) ^ref-25012

---
Very few Amazon executives would have been likely to explicitly declare a policy of “hire the people whom, had they applied ten years ago, would have most strongly resembled the people we did hire back then.” But using language models to filter résumés for “relevance” is making just such a leap. — location: [900](kindle://book?action=open&asin=B085T55LGK&location=900) ^ref-26747

---
Indeed, uncareful deployment of these models might produce a feedback loop from which recovery becomes ever more difficult or requires ever greater interventions. If, say, a résumé-search system detects a gender skew with a given position, and upranks (say) male applicants in a way that exaggerates that skew, then this may well be the next batch of training data on which the model learns. And it will only learn a more extreme version of its existing bias. — location: [912](kindle://book?action=open&asin=B085T55LGK&location=912) ^ref-4979

---
“differential privacy,” which enables companies to collect data about a population of users while maintaining the privacy of the individual users themselves. A web browser company might want to understand user behavior, but without knowing which sites you personally went to; or a smartphone company might want to learn how to improve their spelling-correction or text suggestions without knowing the details of your personal conversations. Differential privacy made this possible. — location: [1143](kindle://book?action=open&asin=B085T55LGK&location=1143) ^ref-59750

---
They spent the entire day talking. By lunchtime, as they sat down together in beloved local restaurant Chez Panisse, they had landed on the topic of fairness. Dwork recalls: “In order that the people around us wouldn’t be disconcerted by our discussions of racism and sexism, we were using terms like ‘purple ties’ and ‘striped shirts,’ and stuff like that. But by lunchtime we were . . . we were on to this.” — location: [1154](kindle://book?action=open&asin=B085T55LGK&location=1154) ^ref-38665

---
