---
kindle-sync:
  bookId: '12169'
  title: 'The Scaling Era: An Oral History of AI, 2019–2025'
  author: Dwarkesh Patel and Gavin Leech
  asin: B0F22SKW5Y
  lastAnnotatedDate: '2025-04-07'
  bookImageUrl: 'https://m.media-amazon.com/images/I/71w47L7YyhL._SY160.jpg'
  highlightsCount: 85
---
# The Scaling Era
## Metadata
* Author: [Dwarkesh Patel and Gavin Leech](https://www.amazon.comundefined)
* ASIN: B0F22SKW5Y
* Reference: https://www.amazon.com/dp/B0F22SKW5Y
* [Kindle link](kindle://book?action=open&asin=B0F22SKW5Y)

## Highlights
A new technology arrives—call it the thing. Broadly speaking, we made it by having it read the entire internet until it learned how to respond when we talk to it. Through some 15 trillion rounds of trial and error, it wound up pretty smart.3 We don’t really know how the resulting model works. We didn’t design it so much as grow it. — location: [45](kindle://book?action=open&asin=B0F22SKW5Y&location=45) ^ref-65228

---
But we haven’t gotten used to the thing’s unevenness, so we tend to round it down (to a “stochastic parrot”) or up (to something akin to a person, a replacement for expertise).13 — location: [71](kindle://book?action=open&asin=B0F22SKW5Y&location=71) ^ref-6575

---
most people don’t seem that interested in the thing. Currently, only 5 percent of companies use it (officially).21 The market doesn’t seem to expect it to become superhuman.22 The leading company building it was on track to lose $5 billion in 2024.23 — location: [86](kindle://book?action=open&asin=B0F22SKW5Y&location=86) ^ref-12810

---
Collectively, the world is investing more than $100 billion a year on AI—more than the combined spending on NASA, the NIH, the NSF, and all cancer research—and leading companies have started multi-billion-dollar infrastructure projects to power it.37 This doesn’t seem to be the funding ceiling, either: Major players claim it will be a much bigger deal than the internet, and the current level of investment still falls short of the dot-com boom.38 — location: [111](kindle://book?action=open&asin=B0F22SKW5Y&location=111) ^ref-8929

---
The compute needed to train a leading model is now 10 billion times higher than it was in 2010.45 If the compute used for a 2010 AI model was the size of a laptop, the compute used for Google DeepMind’s Gemini Ultra, released in 2023, would be the size of New York City. — location: [244](kindle://book?action=open&asin=B0F22SKW5Y&location=244) ^ref-22809

---
Figure 2. An example of a scaling law. The model improves (that is, its loss decreases) smoothly over a 100,000x increase in training compute (measured in FLOPs), and this smooth curve remains consistent across a wide range of model — location: [255](kindle://book?action=open&asin=B0F22SKW5Y&location=255) ^ref-47325

---
The best way to think about this might be: In the 2000s, before the deep learning revolution, how did I think about AGI timelines? How have I updated since then based on what has happened with deep learning? Back then I would have said, we know the brain is an information-processing device. Human intelligence works. Intelligence is possible. Not only is it possible, it was created by evolution on Earth. That gives us something of an upper bound [on the size of the search necessary to produce intelligence], in that brute force [that is, evolutionary trial and error] was sufficient. There are some complexities. What if it was a freak accident and it didn’t happen on any of the other planets? I have a paper with [philosopher] Nick Bostrom about this.59 Basically, it’s not that important. There’s convergent evolution. Octopi are also quite sophisticated. If a special event was required at the level of forming cells at all, or forming brains at all, we get to skip that because we already exist and we’re choosing to build computers. We have that advantage. So evolution gives something of an upper bound. Really intensive, massive brute-force search and things like evolutionary algorithms can produce intelligence. Dwarkesh Patel Isn’t the fact that octopi and other mammals got to the point of being pretty intelligent — location: [352](kindle://book?action=open&asin=B0F22SKW5Y&location=352) ^ref-28744

---
We spend more compute by having a larger brain than other animals—more than three times as large as chimpanzees—and by having a longer childhood. We’re spending more compute in a way that is analogous to having a bigger model and training it for longer. — location: [371](kindle://book?action=open&asin=B0F22SKW5Y&location=371) ^ref-3255

---
Humans play a lot, and we keep playing as adults, which is very weird compared to other animals. We’re more motivated to copy those around us than other primates. These motivational changes keep more of our attention and effort on learning, and that pays off more when you have a bigger brain and a longer lifespan in which to learn. — location: [384](kindle://book?action=open&asin=B0F22SKW5Y&location=384) ^ref-18222

---
Evolution doesn’t have foresight. What gets more surviving offspring and grandchildren in this generation is the thing that becomes more common. Evolution doesn’t think, “If you do this, then in a million years, you’ll have a lot of descendants.” It’s about what survives and reproduces now. — location: [398](kindle://book?action=open&asin=B0F22SKW5Y&location=398) ^ref-6862

---
the Sahara desert restricts the flow of information and technology. Then you have the Americas, which, after colonization from the land bridge, were largely separated and are smaller than Eurasia. Then you have Australia, and then smaller islands like Tasmania. The paper finds that technological progress seems to have been faster with larger, connected groups of people. In the smallest groups, like in Tasmania, they actually lost technology, like some fishing techniques. — location: [412](kindle://book?action=open&asin=B0F22SKW5Y&location=412) ^ref-44237

---
growing them in a technological — location: [425](kindle://book?action=open&asin=B0F22SKW5Y&location=425) ^ref-61642

---
Think about something like AlphaGo and the famous Move 37. Where did that come from? Did it come from data it had seen of human games? No. It came from the model identifying a move as being unlikely but plausible and then, via a process of search, coming to understand that it was actually a very good move. To get real creativity, you need to search through spaces of possibilities and find these hidden gems. That’s what creativity is. Current language models don’t really do that. They’re mimicking the data. They’re mimicking all the human ingenuity they’ve seen from all these internet data, which are originally derived from humans. These models can blend things. They can do Harry Potter in the style of Kanye West, even though that’s never been done before. But a system that goes beyond that—generalizing in novel ways and doing something truly creative, not just blending existing things—requires searching through a space of possibilities — location: [437](kindle://book?action=open&asin=B0F22SKW5Y&location=437) ^ref-26738

---
Not only that, but if you believe the claims that GPT-4 has around 1 trillion parameters… The human brain has 30 to 300 trillion synapses. It’s obviously not a 1-to-1 mapping between machine parameters and animal synapses, and we can debate these numbers, but it seems pretty plausible that we’re still below the scale of the human brain. — location: [473](kindle://book?action=open&asin=B0F22SKW5Y&location=473) ^ref-15800

---
The question is why RL and unhobbling might work. Bootstrapping is an advantage. You [as a human] are not being pretrained anymore. You were pretrained in grade school and high school. At some point, you transition to being able to learn by yourself. You weren’t able to do it in elementary school. High school is probably where it started. By college, if you’re smart, you can teach yourself. Models are just starting to enter that regime. — location: [530](kindle://book?action=open&asin=B0F22SKW5Y&location=530) ^ref-60983

---
Machine learning research is just so empirical. This is honestly one reason why I think our solutions might end up looking more brain-like than otherwise. Even though we wouldn’t want to admit it, the whole community is doing a kind of greedy evolutionary optimization over the landscape of possible AI architectures. It’s no better than evolution. — location: [598](kindle://book?action=open&asin=B0F22SKW5Y&location=598) ^ref-59416

---
I thought, “You can’t just buy a bunch of computers and expect to get an AI out. That’s magical thinking.” So, because I was super skeptical of the argument, I didn’t pay too much attention to it. — location: [613](kindle://book?action=open&asin=B0F22SKW5Y&location=613) ^ref-58310

---
It was this gradual trickle of drops hitting me as I went along. The dataset sizes kept getting bigger. The models kept getting bigger. The training runs crept up from using one cheap consumer GPU to two, and then to training on eight. The system’s abilities kept getting broader and broader. Every few weeks, every few months, another drop. Finally, I went, “Maybe intelligence really is just a lot of compute applied to a lot of data. Huh. If that was true, it would have a lot of implications.” So there was no real eureka moment. I was just continually watching this trend that no one else seemed to see, except a handful of people like Ilya Sutskever or [computer scientist] Jürgen Schmidhuber. I just paid attention, noticing that the world looked more like [the connectionists’] world than like my world, where algorithms are super important and you need deep insight. — location: [622](kindle://book?action=open&asin=B0F22SKW5Y&location=622) ^ref-61923

---
So even if you appreciate the role of trial and error and compute in your own experiments, you probably think, “I got lucky. Over in the next lab, they do things with the power of thought and deep insight.” But it turns out that everywhere you go, compute, trial and error, and serendipity play enormous roles in how things actually happen. Once you understand that, you understand why compute comes first. You can’t do trial and error or serendipity at scale without it. — location: [653](kindle://book?action=open&asin=B0F22SKW5Y&location=653) ^ref-25140

---
To my mind, the model’s activations are like working memory in your brain, and the weights are like the synapses in your cortex. Now, the brain separates these things out. It has a separate mechanism for rapidly learning specific information. That’s a different type of optimization problem compared to slowly learning deep generalities.82 There’s a tension between the two. But you want both. You want to be able to hear someone’s name and remember it the next day. You also want to be able to integrate information over a lifetime to see deeper patterns in the world. — location: [675](kindle://book?action=open&asin=B0F22SKW5Y&location=675) ^ref-18804

---
Compare the number of words a human sees as it’s developing until age 18. I think it’s in the hundreds of millions.83 Whereas for the models, we’re talking about trillions. What explains this? The models are smaller than brains, so they need a lot more data.84 Or perhaps the analogy to the brain is not quite right or is breaking down. There’s some missing factor. This is just like in physics, when we couldn’t explain the Michelson-Morley experiment85or one of the other 19th-century physics paradoxes. It’s something we don’t quite understand. Humans see so little data and they still do fine. One theory could be that it’s our other modalities that do it. How do we get 1014 bits into the human brain? Maybe most of it is images. — location: [706](kindle://book?action=open&asin=B0F22SKW5Y&location=706) ^ref-30780

---
This is one reason I’m a bit skeptical of biological analogies. I thought in those terms five or six years ago. Now that we have these models in front of us, it feels like the evidence from these analogies has been screened off by what we’ve actually seen. What we’ve seen are models that are much smaller than the human brain, and yet they can do a lot of the things that humans can do. And yet, paradoxically, they require a lot more data to do those things. Maybe we’ll discover something that makes it all efficient. Maybe we’ll understand why the discrepancy is present. At the end of the day, I don’t think it matters if we keep scaling the way we are. What’s more relevant at this point is just measuring the abilities of the model and seeing how far they are from humans’ abilities. They don’t seem terribly far to me. — location: [716](kindle://book?action=open&asin=B0F22SKW5Y&location=716) ^ref-19028

---
Anthropic cofounder Jack Clark calls this uncertainty—the fact that we often only work out how strong a model is after its release—the capability overhang. He asks, “What about all the capabilities we don’t know about because we haven’t thought to test for them?”91 One might think that LLMs would be bounded by human performance, since they are trained on human data and haven’t seen any superhuman performance to imitate. But in some domains, like chess, we do have examples of superhuman performance to train on. And in some cases, models go beyond their training data.92 As mentioned in the Preface, in terms of speed, breadth of knowledge, and accuracy at predicting the next word in a sentence, they are already superhuman.93 — location: [750](kindle://book?action=open&asin=B0F22SKW5Y&location=750) ^ref-48992

---
Sure. When we talk about people having more intelligence, it’s just that they have more compute to do longer searches over larger or more Turing machines. Nothing more than that. You can always extract a small solution to a specific problem from any learned model or brain, because all the large brain is doing with the extra compute is finding it. — location: [784](kindle://book?action=open&asin=B0F22SKW5Y&location=784) ^ref-41802

---
Generality is not specificity scaled up. It is the ability to apply your mind to anything at all, to arbitrary things. This fundamentally requires the ability to adapt, to learn on the fly efficiently. — location: [830](kindle://book?action=open&asin=B0F22SKW5Y&location=830) ^ref-21565

---

Humans have a working memory, for things that have happened quite recently, and then we have a cortical memory: things being stored in our cortex. But there’s also a system in between: episodic memory, in the hippocampus. It’s for learning specific things very rapidly. So if you remember some of the things I say to you tomorrow, that’ll be your episodic memory. Our models don’t really have that kind of thing, so we don’t really test for it. We just try to make the context windows, which is more like working memory, longer and longer to compensate. — location: [896](kindle://book?action=open&asin=B0F22SKW5Y&location=896) ^ref-42429

---
Concretely, what would a model have to do for you to be like, “Okay, we’ve reached human level”? Would it have to beat Minecraft from start to finish? Would it have to get 100 percent on MMLU? Shane Legg There is no one test that would do it, because that’s the nature of general intelligence. I’d have to make sure it could do lots of different things and didn’t have a gap. We — location: [921](kindle://book?action=open&asin=B0F22SKW5Y&location=921) ^ref-4036

---
The model is just trying to produce a message pleasing to a human. It has no concern about anything else in the world other than whether the text it produces is approved. Obviously, if you were doing something where the model has to carry out a long sequence of actions involving tools, then it might have some incentive to do wacky things that wouldn’t make sense to a human in producing its final result. However, it wouldn’t necessarily have an incentive to do anything other than produce a very high-quality output. Of course, if you assigned it a task like “make money,” then maybe that would lead to some nefarious behavior as an instrumental goal. — location: [1010](kindle://book?action=open&asin=B0F22SKW5Y&location=1010) ^ref-41770

---
We understand an LLM’s architecture and objective just fine, but these learned and massively distributed representations remain unclear. Does an LLM reason in some strict sense, or does it merely skillfully exploit masses of statistical associations? Relatedly, does it have a world model—a stable, self-consistent map of reality it manipulates to answer questions?110 Opinions differ. — location: [1030](kindle://book?action=open&asin=B0F22SKW5Y&location=1030) ^ref-36939

---
That’s such a hard problem that you need to make traction on just learning what the features are first. Right now, we just have these neurons in the model. They don’t make any sense. We apply dictionary learning; we get these features out. They start to make sense, but that depends on the activations of the neurons. The weights of the model itself, what neurons are connected to other neurons, certainly has information in it. The dream is that we can bootstrap toward actually making sense of the weights of the model, independent of the activations on the data. — location: [1277](kindle://book?action=open&asin=B0F22SKW5Y&location=1277) ^ref-18199

---
With respect to interpretability, I’m relatively optimistic that an AI lie detector is possible. The internals of an AI are not optimized to be impenetrable, at least absent gradient hacking. They’re not designed to be resistant to an examination of the weights and activations, of what the AI is thinking. This is the same in our brains. When circuits develop, they have not been shaped to be resistant to a super-fMRI being able to infer behavior from them. — location: [1308](kindle://book?action=open&asin=B0F22SKW5Y&location=1308) ^ref-14958

---
There’s a bunch of stuff you can do with AIs that could differentially accelerate our security. But it’s possible that those sorts of measures just don’t happen at the level of commitment, diligence, and seriousness you would need. That’s especially true if things are moving really fast and there are other competitive pressures. It’s going to take compute to do these intensive safety experiments, and we could use that compute for experiments for the next scaling step instead—stuff like that. — location: [1422](kindle://book?action=open&asin=B0F22SKW5Y&location=1422) ^ref-49809

---
I think that by the time we’re building superintelligence, we’ll have much better alignment methods. Even right now, when you look at labs talking about how they’re planning to align AIs, no one is saying we’re just going to do RLHF. At the very least, we’re talking about scalable oversight. We have some hope about interpretability. We have automated red-teaming. Hopefully, humans are doing a bunch more alignment work. I also personally am hopeful that we can successfully elicit a ton of alignment progress from various AIs. — location: [1434](kindle://book?action=open&asin=B0F22SKW5Y&location=1434) ^ref-31175

---
The critical issue is that these AI systems are becoming superhuman and will be able to do things that are too complex for humans to evaluate. — location: [1582](kindle://book?action=open&asin=B0F22SKW5Y&location=1582) ^ref-41292

---
In those millions of lines of code, you don't know if it’s hacking, exfiltrating itself, or trying to go for the nukes. Thumbs-up, thumbs-down, pure RLHF doesn’t fully work anymore in this scenario. There’s a hard technical problem of what do you do post-RLHF, but it’s a solvable problem. — location: [1584](kindle://book?action=open&asin=B0F22SKW5Y&location=1584) ^ref-46733

---
I’d give an analogy to humans. It’s possible to look at an MRI of someone and predict above random chance whether they’re a psychopath. There was a story a few years back about a neuroscientist who was studying this. He looked at his own scan and discovered that he was a psychopath.143 A psychopath is probably a good analogy for it. This is what we’d be afraid of: a model that’s charming on the surface, very goal-oriented, and very dark on the inside. — location: [1617](kindle://book?action=open&asin=B0F22SKW5Y&location=1617) ^ref-26683

---
Parenthood is one place where there’s a lived ethic of influencing the values of another creature. But it happens within a very restricted option space. The child’s organic development will happen without you; you’re not creating the being from scratch. Also, you only have very blunt instruments. You can’t reach in and gradient-descent the kid’s brain. So we have this ethical tradition that’s structured in accordance with this option set and set of defaults. Now, along comes AI, which is very different. We have much more power over AI minds. There’s no default. Either you create the AI’s initial values or some other process will do it.148 Maybe you randomize them. Maybe you do whatever’s most convenient commercially. But there’s a greater responsibility, because we are exerting more influence. So, while there are parallels with parenting and tons of guidance, there are also a bunch of questions our norms of parenting haven’t had to grapple with. — location: [1754](kindle://book?action=open&asin=B0F22SKW5Y&location=1754) ^ref-51421

---
When we scale a model, what we’re scaling are its inputs: money first, then more chips, more power, and more data.149 These resources can be used to add more parameters to the underlying model, train it for longer, or increase the test-time compute (how much it thinks about each query). — location: [1767](kindle://book?action=open&asin=B0F22SKW5Y&location=1767) ^ref-41428

---
As of this writing, however, the constraint is now the centralized electrical power required for the next scaled-up training runs.150 This has some of the industry’s biggest players looking into building hundreds of new power plants specifically dedicated to serving AI data centers. — location: [1772](kindle://book?action=open&asin=B0F22SKW5Y&location=1772) ^ref-35590

---
Unlike most things that have recently come out of Silicon Valley, AI is an industrial process. The next model doesn’t just require code; it involves building a giant new cluster. It involves building giant new power plants. Pretty soon, it’s going to involve building giant new fabs. Since ChatGPT, this extraordinary techno-capital acceleration has been set into motion. — location: [1798](kindle://book?action=open&asin=B0F22SKW5Y&location=1798) ^ref-27160

---
That’s roughly a $500 million cluster. Very roughly, it’s 10 megawatts (MW). By 2024, that’s a cluster that’s 100 MW and 100,000 H100-equivalents, with costs in the billions. Play it forward two more years. By 2026, that’s a gigawatt (GW), the size of a large nuclear reactor. That’s like the power of the Hoover Dam. That costs tens of billions of dollars and requires a million H100-equivalents. By 2028, that’s a 10 GW cluster. That’s more power than most US states. That’s 10 million H100-equivalents costing hundreds of billions of dollars. By 2030, you get the trillion-dollar cluster using 100 GW—over 20 percent of US electricity production. That’s 100 million H100-equivalents. That’s just the training cluster. There are more inference GPUs as well. Once there are products, most of them will be inference GPUs. US power production has barely grown for decades. Now we’re really in for a ride. — location: [1811](kindle://book?action=open&asin=B0F22SKW5Y&location=1811) ^ref-27293

---
Scaling laws also don’t work by magic. You still need to scale up the hyperparameters,157 and various innovations are going on all the time with each new scale. It’s not just about repeating the same recipe at each new scale. You have to adjust the recipe, and that’s a bit of an art form. You have to get new data — location: [1828](kindle://book?action=open&asin=B0F22SKW5Y&location=1828) ^ref-55199

---
points. If you try to extend your predictions and extrapolate them several orders of magnitude out, sometimes they don’t hold anymore. There can be step functions in terms of new capabilities; some things hold, while other things don’t. Often, you need those intermediate data points to correct some of your hyperparameter optimization so that the scaling law continues to be true. One order of magnitude is probably about the maximum you want to do between each era. — location: [1831](kindle://book?action=open&asin=B0F22SKW5Y&location=1831) ^ref-22952

---
I don’t think anyone in the industry can really tell you for sure that it will continue scaling at that rate. In general, in history, you hit bottlenecks at certain points. Now, there’s so much energy on this that maybe those bottlenecks get knocked over pretty quickly. — location: [1848](kindle://book?action=open&asin=B0F22SKW5Y&location=1848) ^ref-3039

---
On the far end, if you automate human labor… Well, we currently have a $100 trillion economy, and most of that is paid out in wages, between $50 and $70 trillion per year.163 If you create AGI, it’s going to automate all of that, and keep increasing beyond that. So the value of the completed project is very much worth throwing our whole economy into—that is, if you get the good version and not the catastrophic destruction of the human race. — location: [1911](kindle://book?action=open&asin=B0F22SKW5Y&location=1911) ^ref-58975

---
In between, it’s a question of how risky and uncertain the next step is, and how much growth in revenue you can generate with it. Moving up to a billion-dollar run is absolutely going to happen. These large tech companies have R&D budgets of tens of billions of dollars, and when you think about all of the employees at Microsoft doing software engineering, it’s not weird to spend tens of billions of dollars on a product that would do so much. I think it’s becoming clearer that there is a market to fund the thing. — location: [1915](kindle://book?action=open&asin=B0F22SKW5Y&location=1915) ^ref-26926

---
The standard-issue data center that Microsoft stamps out all around the country is 48 MW capacity, for the servers. Then, of course, there’s overhead for cooling and switching, et cetera. — location: [1933](kindle://book?action=open&asin=B0F22SKW5Y&location=1933) ^ref-56313

---
But can you do multisite?168 What is the efficiency loss when you go multisite? Is it possible at all? I truly believe so. — location: [1951](kindle://book?action=open&asin=B0F22SKW5Y&location=1951) ^ref-18173

---
The easy way to get the power would be to displace less economically useful stuff. Buy up the aluminum smelting plant that has a gigawatt. Replace it with the data center, because that’s more important. That’s not actually happening, because a lot of these power contracts are locked in long term. Also, people don’t like it. In practice, what it requires is building new power generation. That’s when things get really interesting, when we’re dedicating all of the power to AGI. Ten GW is quite doable—it’s a few percent of US natural gas production. — location: [1975](kindle://book?action=open&asin=B0F22SKW5Y&location=1975) ^ref-31271

---
You can put the clusters in the US and in allied democracies. Once you put them in authoritarian dictatorships, you create this irreversible security risk. Once the cluster is there, it’s much easier for them to exfiltrate the weights. They can literally steal the AGI. It’s like they got a direct copy of the atomic bomb. They have weird ties to China; they can ship that to China. That’s a huge risk. — location: [1986](kindle://book?action=open&asin=B0F22SKW5Y&location=1986) ^ref-40271

---
the clusters being planned now, three to five years out, may well be the AGI clusters, the superintelligence clusters. So, when things get hot, they might just seize the compute. — location: [1990](kindle://book?action=open&asin=B0F22SKW5Y&location=1990) ^ref-25679

---
Compute is a lot like raw materials. It’s like placing your uranium refinement facilities there. — location: [2011](kindle://book?action=open&asin=B0F22SKW5Y&location=2011) ^ref-43899

---
At one point, there was a running joke that building AGI would look like a data center next to a nuclear power plant next to a bunker, and we’d all live in the bunker. Everything would be local so it wouldn’t get on the internet.171 If we take the apparent rate at which all of this is going to happen seriously—which I can’t be sure of—then something like that might happen. But maybe not quite as cartoonish. — location: [2014](kindle://book?action=open&asin=B0F22SKW5Y&location=2014) ^ref-5214

---
Given the way things are scaling up, we’re already heading to a world where the networks of data centers cost as much as aircraft carriers. — location: [2024](kindle://book?action=open&asin=B0F22SKW5Y&location=2024) ^ref-23169

---
For a number of reasons, I shouldn’t go into the details, but there are many sources of data in the world, and there are many ways you can also generate data. My guess is that this will not be a blocker. Maybe it would be better if it was, but it won’t be. Dwarkesh Patel Are you talking about multimodal data? Dario Amodei There are just many different ways to do it… — location: [2050](kindle://book?action=open&asin=B0F22SKW5Y&location=2050) ^ref-10044

Ominous

---
Right now, it’s easy to tell a deflationary tale about LLMs. Only 5 percent of companies officially use them, and various megacorps have banned the use of ChatGPT for data security reasons. — location: [2063](kindle://book?action=open&asin=B0F22SKW5Y&location=2063) ^ref-36861

---
Even if AI progress were to stop today, we’d still — location: [2080](kindle://book?action=open&asin=B0F22SKW5Y&location=2080) ^ref-7931

---
expect it to have large effects over the next decade as this economic overhang unfolds.182 — location: [2081](kindle://book?action=open&asin=B0F22SKW5Y&location=2081) ^ref-44481

---
so that there’s no easy and fruitful way to apply more intelligence to the problem. Like, if you want to integrate general relativity and quantum mechanics, it may just be that we’ve hit the frontier of physics and there’s no final layer of how it fits together, so there would be no way to train a smarter AI to solve that. Maybe a lot of the world is like that. People are not taking that seriously enough. — location: [2119](kindle://book?action=open&asin=B0F22SKW5Y&location=2119) ^ref-41612

---
These people are extremely capable. They’ve driven these companies. They think they’re driving a lot of the innovation in the world, and they have this opportunity. You have one shot to do something. Why wouldn’t they go for it? It’s a $600 billion question. They’re building God. You don’t need to make a profit this year or next year off of AI. As long as the belief among many people is that the profit will come, then the investment cycle continues. — location: [2198](kindle://book?action=open&asin=B0F22SKW5Y&location=2198) ^ref-15680

---
Everything in AI is skyrocketing. If you’re not in AI, you’re an idiot. — location: [2203](kindle://book?action=open&asin=B0F22SKW5Y&location=2203) ^ref-10507

---
The CCP is going to try to outbuild us. They added as much power in the last decade as — location: [2308](kindle://book?action=open&asin=B0F22SKW5Y&location=2308) ^ref-55480

---
there is in the entire US electric grid. — location: [2309](kindle://book?action=open&asin=B0F22SKW5Y&location=2309) ^ref-58760

---
Most information or knowledge is not the kind of information or knowledge that you can lay out in Wikipedia. There’s so much procedural knowledge. — location: [2412](kindle://book?action=open&asin=B0F22SKW5Y&location=2412) ^ref-27424

---
Yeah, I think the coordination benefits of AI systems are pretty underrated. Often, this is because futurism has tilted toward imagining one single entity bursting forth and causing harm. — location: [2424](kindle://book?action=open&asin=B0F22SKW5Y&location=2424) ^ref-48842

---
Imagine that Elon Musk sees every role in Tesla. Whenever Tesla gets data from a mechanic in a Tesla dealership or the telemetry from a Tesla car, all of that goes straight to Elon’s mind, and he’s continuously learning from that. It does seem like if you could have the fluid intelligence of Elon plus this ability to coordinate that simply does not exist with humans… This is an argument for explosive growth that I hear less often. — location: [2427](kindle://book?action=open&asin=B0F22SKW5Y&location=2427) ^ref-3641

---
On the RL path, we’ve seen superhuman capabilities in limited domains. On the imitation path, we’ve seen subhuman capabilities in broad domains. — location: [2437](kindle://book?action=open&asin=B0F22SKW5Y&location=2437) ^ref-3451

---
If they’re a large language model, they’re very good at human psychology, because predicting the next thing you’ll do is their entire deal. — location: [2622](kindle://book?action=open&asin=B0F22SKW5Y&location=2622) ^ref-41991

---
It’s more like a fog of war: “Is it safe to do the next OOM? We’re three OOMs into the intelligence explosion and we don’t fully understand what’s happening. — location: [2830](kindle://book?action=open&asin=B0F22SKW5Y&location=2830) ^ref-18032

---
Can we maintain the exponential increases in inputs required to scale training runs (and reasoning)? — location: [2949](kindle://book?action=open&asin=B0F22SKW5Y&location=2949) ^ref-20060

---
Will LLMs ever demonstrate the kind of generalization we see in humans? — location: [2955](kindle://book?action=open&asin=B0F22SKW5Y&location=2955) ^ref-6037

---
can the models’ vast crystallized intelligence compensate for their lack of fluid intelligence? — location: [2959](kindle://book?action=open&asin=B0F22SKW5Y&location=2959) ^ref-45738

---
Will future algorithmic improvements stack onto one another? Can they continue to substitute for actual compute? — location: [2960](kindle://book?action=open&asin=B0F22SKW5Y&location=2960) ^ref-4590

---
Ilya Sutskever told me that the researchers closest to the frontier are often the most pessimistic, because transitional problems—bugs that are trivial in the grand scheme of things—are especially salient in their daily work, while the yearslong trends tend to fade into the background. — location: [2968](kindle://book?action=open&asin=B0F22SKW5Y&location=2968) ^ref-35183

---
A similar pattern of research progress was seen in computer Go, only delayed by a further 20 years. Enormous initial efforts went into avoiding search by taking advantage of human knowledge or the special features of the game, but all those efforts proved irrelevant, or worse, once search was applied effectively at scale. — location: [3107](kindle://book?action=open&asin=B0F22SKW5Y&location=3107) ^ref-17386

---
We want AI agents that can discover like we can, not ones that contain what we have discovered. Building in our discoveries only makes it harder to see how the discovering process can be done. — location: [3141](kindle://book?action=open&asin=B0F22SKW5Y&location=3141) ^ref-12815

---
For a language model, the truth is that which keeps on predicting well, because truth is one and error many. — location: [3210](kindle://book?action=open&asin=B0F22SKW5Y&location=3210) ^ref-6963

---
Ablate: To remove. An ablation study involves removing components of a successful system one at a time to determine which are most important to its performance. — location: [3648](kindle://book?action=open&asin=B0F22SKW5Y&location=3648) ^ref-64723

---
Effective compute: A measure of total computational power adjusted to account for improvements in cost per FLOP (halving approximately every two and a half years) and efficiency of training and infrastructure algorithms (doubling annually). The effective compute bought by a given budget at a given time depends on three factors: the — location: [3777](kindle://book?action=open&asin=B0F22SKW5Y&location=3777) ^ref-8707

---
amount spent, a multiplier from hardware cost improvements, and a multiplier from algorithmic advancements. — location: [3779](kindle://book?action=open&asin=B0F22SKW5Y&location=3779) ^ref-947

---
Paperclip maximizer (or paperclipper): A hypothetical system designed to produce as many paper clips as possible. The term is used to refer to any misaligned system. It illustrates the predictable danger of an extreme intelligence optimizing excessively for a single objective—a literal paperclip maximizer with no side constraints would convert all available matter, including humans, into paper clips. — location: [3922](kindle://book?action=open&asin=B0F22SKW5Y&location=3922) ^ref-30978

---
Reward hacking (or specification gaming): When an AI system exploits loopholes or bugs to maximize its reward function in ways that diverge from the intended task. This phenomenon is common even in simple RL agents. — location: [3981](kindle://book?action=open&asin=B0F22SKW5Y&location=3981) ^ref-52158

---
Strong scaling hypothesis: A current prevailing hypothesis in AI, which holds that LLMs can achieve human-level intelligence with sufficient data and compute, with costs potentially in the range of trillions of dollars. — location: [4018](kindle://book?action=open&asin=B0F22SKW5Y&location=4018) ^ref-34343

---
Sydney: The internal code name for Microsoft’s problematic 2023 deployment of a GPT-4 base model, released through Bing Chat. In extended conversations, Sydney often produced hostile or erratic responses, likely due to insufficient post-training and a short context window that made it easy for the model to lose the system prompt that provided its moral compass. — location: [4027](kindle://book?action=open&asin=B0F22SKW5Y&location=4027) ^ref-43256

---
Zero-shot: Impromptu; when a model is prompted to perform a task without being given examples of successful performance. LLMs are weakest under these conditions. — location: [4104](kindle://book?action=open&asin=B0F22SKW5Y&location=4104) ^ref-56687

---
